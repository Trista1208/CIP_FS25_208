{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -r requrirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all imports. use \"pip install -r requrirements.txt\" ^^ if you don't have the needed librarys installed\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import etree\n",
    "import re\n",
    "import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start Chrome\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\")\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "url = \"https://www.galaxus.ch/en/s2/producttype/robot-vacuum-cleaners-174?take=204\"\n",
    "\n",
    "driver.get(url)\n",
    "\n",
    "\n",
    "time.sleep(random.uniform(4, 5))\n",
    "# scroll down for the lazy loaded items\n",
    "scroll_height = 0\n",
    "scroll_step = 4000\n",
    "new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "while True:\n",
    "    # scrolling\n",
    "    driver.execute_script(f\"window.scrollTo({scroll_height}, {scroll_height + scroll_step});\")\n",
    "    time.sleep(random.uniform(0.3, 0.6))\n",
    "    scroll_height += scroll_step\n",
    "    \n",
    "    # if it gets to the bottom\n",
    "    if new_height < scroll_height:\n",
    "        try:\n",
    "            # choose the button to click\n",
    "            show_more = WebDriverWait(driver, 2).until(EC.element_to_be_clickable((By.XPATH, '//button[contains(@class, \"productListFooter_styled_StyledLoadMoreButton\")]')))\n",
    "            # scroll to the right place to press the button\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", show_more)\n",
    "            time.sleep(random.uniform(0.3, 0.6))\n",
    "            show_more.click()\n",
    "            print(\"clicked a button\")\n",
    "            # in case the scrolling window gets bigger\n",
    "            old_height = int(new_height)\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            time.sleep(random.uniform(1.3, 1.6))\n",
    "            # savety against a inf loop\n",
    "            if old_height == new_height:\n",
    "                break\n",
    "        except:\n",
    "            # normal out\n",
    "            print(\"no more buttons are found\")\n",
    "            break\n",
    "    \n",
    "# save the HTML to search it\n",
    "soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "urls = list()\n",
    "\n",
    "# put all links in a list\n",
    "for a in soup.find_all(\"a\", href=True):\n",
    "    href = a[\"href\"]\n",
    "    # only products\n",
    "    if href.startswith(\"/en/s2/product/\"):\n",
    "        full_url = \"https://www.galaxus.ch\" + href\n",
    "        if full_url not in urls:\n",
    "            urls.append(full_url)\n",
    "\n",
    "# for testing\n",
    "print(len(urls))\n",
    "print(urls)\n",
    "    \n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Chrome\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\")\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "#just for testing\n",
    "# urls = [\"https://www.galaxus.ch/en/s2/product/roborock-s8-maxv-ultra-vacuum-mopping-robot-robot-vacuum-cleaners-43695849\", \"https://www.galaxus.ch/en/s2/product/dreame-x50-ultra-complete-vacuum-mopping-robot-robot-vacuum-cleaners-53898301\"]\n",
    "\n",
    "for url in tqdm.notebook.tqdm(urls[60:]):\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "    dom = etree.HTML(str(soup))\n",
    "\n",
    "    # here we tryed to do things with the XPATH. Mabe it would have been better with only soup, because there are a lot of trys.\n",
    "    # but we tought that the base product things are always on the same place (which was wrong)\n",
    "    # extracting the name\n",
    "    name_element = dom.xpath('//*[@id=\"pageContent\"]/div/div[1]/div/div/div[2]/div/div[1]/div/h1')\n",
    "    name = name_element[0].xpath(\"string()\")\n",
    "\n",
    "\n",
    "    # extracting the price\n",
    "    try:\n",
    "        price_element = dom.xpath('//*[@id=\"pageContent\"]/div/div[1]/div/div/div[2]/div/div[1]/span/strong/button/text()')\n",
    "        price_float = float(re.sub(r\"[^\\d]\", \"\", price_element[0]))\n",
    "    except:\n",
    "        # can happen if the product is returned\n",
    "        price_element = dom.xpath('//*[@id=\"pageContent\"]/div/div[1]/div[1]/div/div[2]/div/div[1]/span/strong/text()')\n",
    "        price_float = float(re.sub(r\"[^\\d]\", \"\", price_element[0]))\n",
    "    \n",
    "    if data[name] is None:\n",
    "        data[name] = dict()\n",
    "    else:\n",
    "        # if theoretical only the color changes, we just overwrite the old datapoint. But they would be simmilar anyway\n",
    "        # rename the old one\n",
    "        data[name + \"_\" + data[name][\"price\"]] = data.pop(name)\n",
    "        # create the new one\n",
    "        data[name + \"_\" + price_float] = dict()\n",
    "        \n",
    "    data.get(name).update({\"price\" : price_float})\n",
    "\n",
    "    # extracting the rating\n",
    "    try:\n",
    "        rating_element = dom.xpath('//*[@id=\"pageContent\"]/div/div[1]/div/div/div[2]/div/div[2]/div[1]/a/span[3]')                   \n",
    "        rating_float = float(rating_element[0].get(\"aria-label\").split()[0])\n",
    "    except:\n",
    "        try:\n",
    "            # only a small change, I thing it happens when there is an additional price reduction window\n",
    "            rating_element = dom.xpath('//*[@id=\"pageContent\"]/div/div[1]/div/div/div[2]/div/div[3]/div[1]/a/span[3]')\n",
    "            rating_float = float(rating_element[0].get(\"aria-label\").split()[0])\n",
    "        except:\n",
    "            # happens if there is no rating\n",
    "            rating_float = None\n",
    "    data.get(name).update({\"rating\" : rating_float})\n",
    "\n",
    "    # extracting the rating\n",
    "    try:\n",
    "        rating_count_element = dom.xpath('//*[@id=\"pageContent\"]/div/div[1]/div/div/div[2]/div/div[2]/div[1]/a/span[1]')\n",
    "        rating_count_int = int(rating_count_element[0].xpath(\"string()\"))\n",
    "    except:\n",
    "        try:\n",
    "            # only a small change, I thing it happens when there is an additional price reduction window\n",
    "            rating_count_element = dom.xpath('//*[@id=\"pageContent\"]/div/div[1]/div/div/div[2]/div/div[3]/div[1]/a/span[1]')\n",
    "            rating_count_int = int(rating_count_element[0].xpath(\"string()\"))\n",
    "        except:\n",
    "            # happens if there is no rating\n",
    "            rating_float = None\n",
    "    data.get(name).update({\"rating_count\" : rating_count_int})\n",
    "\n",
    "    # here we did it only with soup\n",
    "    # save everything in the tables\n",
    "    for table in soup.find_all(class_=\"specificationTable_styled_SpecificationTableStyled__G_W91\"):\n",
    "        table_name = table.find(\"caption\").get_text(strip=True) if table.find(\"caption\") else \"Unknown\"\n",
    "        for row in table.find_all(\"tr\"):\n",
    "            cells = row.find_all(\"td\")\n",
    "            if len(cells) == 2:\n",
    "                key = cells[0].get_text(separator=\" \", strip=True)\n",
    "                values = cells[1].find_all(\"span\", class_=\"value_StyledValue__KCmTz\")\n",
    "                value_texts = [v.get_text(\" \", strip=True) for v in values]\n",
    "                value = \", \".join(value_texts).replace(\"\\xa0\", \" \")\n",
    "                data.get(name).update({table_name + \"  \" + key : value})\n",
    "    driver.delete_all_cookies()\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of the dict\n",
    "print(list(data.keys())[0])\n",
    "data[list(data.keys())[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the dict to DataFrame and put the name in a new line\n",
    "df = pd.DataFrame.from_dict(data, orient='index')\n",
    "df = df.reset_index().rename(columns={\"index\": \"product_name\"})\n",
    "print(df.shape)\n",
    "# save as csv\n",
    "df.to_csv(\"robot_vacuums.csv\", index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".CIP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
