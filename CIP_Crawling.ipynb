{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all imports. use \"pip install -r requirements.txt\" ^^ if you don't have the needed librarys installed\n",
    "import undetected_chromedriver as uc\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import etree\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start \"undetectable\" Chrome\n",
    "options = uc.ChromeOptions()\n",
    "options.add_argument(\"--no-first-run --no-service-autorun --password-store=basic\")\n",
    "options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36\")\n",
    "\n",
    "# to make shure we don't get detected as a bot\n",
    "driver = uc.Chrome(options=options, headless=False)\n",
    "\n",
    "url = \"https://www.galaxus.ch/en/s2/producttype/robot-vacuum-cleaners-174?take=204\"\n",
    "driver.get(url)\n",
    "\n",
    "# Warte auf erste Seite\n",
    "time.sleep(random.uniform(5, 7))\n",
    "\n",
    "# scroll down for the lazy loaded items\n",
    "scroll_height = 0\n",
    "scroll_step = 2000\n",
    "new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "while True:\n",
    "    # scrolling\n",
    "    driver.execute_script(f\"window.scrollTo({scroll_height}, {scroll_height + scroll_step});\")\n",
    "    time.sleep(random.uniform(0.2, 0.4))\n",
    "    scroll_height += scroll_step\n",
    "\n",
    "    # if it gets to the bottom\n",
    "    if new_height < scroll_height:\n",
    "        try:\n",
    "            # choose the button to click\n",
    "            show_more = WebDriverWait(driver, 3).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, '//button[contains(@class, \"productListFooter_styled_StyledLoadMoreButton\")]'))\n",
    "            )\n",
    "            # scroll to the right place to press the button\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", show_more)\n",
    "            time.sleep(random.uniform(0.4, 0.8))\n",
    "            show_more.click()\n",
    "            print(\"Clicked button\")\n",
    "            # in case the scrolling window gets bigger\n",
    "            old_height = new_height\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            time.sleep(random.uniform(1.5, 2.5))\n",
    "\n",
    "            # savety against a inf loop\n",
    "            if old_height == new_height:\n",
    "                # this happens if you are detected as a bot\n",
    "                print(\"Button clicked but nothing changed.\")\n",
    "                break\n",
    "        except:\n",
    "            # normal out\n",
    "            print(\"No more buttons found.\")\n",
    "            break\n",
    "\n",
    "# save the HTML to search it\n",
    "soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "urls = []\n",
    "\n",
    "# put all links in a list\n",
    "for a in soup.find_all(\"a\", href=True):\n",
    "    href = a[\"href\"]\n",
    "    # only products\n",
    "    if href.startswith(\"/en/s2/product/\"):\n",
    "        full_url = \"https://www.galaxus.ch\" + href\n",
    "        if full_url not in urls:\n",
    "            urls.append(full_url)\n",
    "\n",
    "# for testing\n",
    "print(f\"\\nðŸ”Ž Found {len(urls)} product URLs.\")\n",
    "for u in urls[:5]: print(u)\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start \"undetectable\" Chrome\n",
    "options = uc.ChromeOptions()\n",
    "options.add_argument(\"--no-first-run --no-service-autorun --password-store=basic\")\n",
    "options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36\")\n",
    "\n",
    "# to make shure we don't get detected as a bot\n",
    "driver = uc.Chrome(options=options, headless=False)\n",
    "\n",
    "#just for testing\n",
    "# urls = [\"https://www.galaxus.ch/en/s2/product/roborock-s8-maxv-ultra-vacuum-mopping-robot-robot-vacuum-cleaners-43695849\", \"https://www.galaxus.ch/en/s2/product/dreame-x50-ultra-complete-vacuum-mopping-robot-robot-vacuum-cleaners-53898301\"]\n",
    "\n",
    "for i, url in tqdm(enumerate(urls), total=len(urls)):\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(random.uniform(3, 7))\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "        dom = etree.HTML(str(soup))\n",
    "\n",
    "        # here we tryed to do things with the XPATH. Mabe it would have been better with only soup, because there are a lot of trys.\n",
    "        # but we tought that the base product things are always on the same place (which was wrong)\n",
    "        # extracting the name\n",
    "\n",
    "        name_element = dom.xpath('//*[@id=\"pageContent\"]/div/div[1]/div/div/div[2]/div/div[1]/div/h1')\n",
    "        name = name_element[0].xpath(\"string()\")\n",
    "\n",
    "\n",
    "\n",
    "        # extracting the price\n",
    "        try:\n",
    "            price_element = dom.xpath('//*[@id=\"pageContent\"]/div/div[1]/div/div/div[2]/div/div[1]/span/strong/button/text()')\n",
    "            price_float = float(re.sub(r\"[^\\d]\", \"\", price_element[0]))\n",
    "        except:\n",
    "            # can happen if the product is returned\n",
    "            price_element = dom.xpath('//*[@id=\"pageContent\"]/div/div[1]/div[1]/div/div[2]/div/div[1]/span/strong/text()')\n",
    "            price_float = float(re.sub(r\"[^\\d]\", \"\", price_element[0]))\n",
    "        \n",
    "        \n",
    "        if data.get(str(name)) is None:\n",
    "            data[str(name)] = dict()\n",
    "        else:\n",
    "            # if theoretical only the color changes, we just overwrite the old datapoint. But they would be simmilar anyway\n",
    "            # rename the old one\n",
    "            data[name + \"_\" + str(data.get(name).get(\"price\"))] = data.get(name)\n",
    "            data.pop(name)\n",
    "            # create the new one\n",
    "            name = name + \"_\" + str(price_float)\n",
    "            data[name] = dict()\n",
    "            \n",
    "        data.get(name).update({\"price\" : price_float})\n",
    "\n",
    "        # extracting the rating\n",
    "        try:\n",
    "            rating_element = dom.xpath('//*[@id=\"pageContent\"]/div/div[1]/div/div/div[2]/div/div[2]/div[1]/a/span[3]')                   \n",
    "            rating_float = float(rating_element[0].get(\"aria-label\").split()[0])\n",
    "        except:\n",
    "            try:\n",
    "                # only a small change, I thing it happens when there is an additional price reduction window\n",
    "                rating_element = dom.xpath('//*[@id=\"pageContent\"]/div/div[1]/div/div/div[2]/div/div[3]/div[1]/a/span[3]')\n",
    "                rating_float = float(rating_element[0].get(\"aria-label\").split()[0])\n",
    "            except:\n",
    "                # happens if there is no rating\n",
    "                rating_float = None\n",
    "        data.get(name).update({\"rating\" : rating_float})\n",
    "\n",
    "        # extracting the rating\n",
    "        try:\n",
    "            rating_count_element = dom.xpath('//*[@id=\"pageContent\"]/div/div[1]/div/div/div[2]/div/div[2]/div[1]/a/span[1]')\n",
    "            rating_count_int = int(rating_count_element[0].xpath(\"string()\"))\n",
    "        except:\n",
    "            try:\n",
    "                # only a small change, I thing it happens when there is an additional price reduction window\n",
    "                rating_count_element = dom.xpath('//*[@id=\"pageContent\"]/div/div[1]/div/div/div[2]/div/div[3]/div[1]/a/span[1]')\n",
    "                rating_count_int = int(rating_count_element[0].xpath(\"string()\"))\n",
    "            except:\n",
    "                # happens if there is no rating\n",
    "                rating_float = None\n",
    "        data.get(name).update({\"rating_count\" : rating_count_int})\n",
    "\n",
    "        # here we did it only with soup\n",
    "        # save everything in the tables\n",
    "        for table in soup.find_all(class_=\"specificationTable_styled_SpecificationTableStyled__G_W91\"):\n",
    "            table_name = table.find(\"caption\").get_text(strip=True) if table.find(\"caption\") else \"Unknown\"\n",
    "            for row in table.find_all(\"tr\"):\n",
    "                cells = row.find_all(\"td\")\n",
    "                if len(cells) == 2:\n",
    "                    key = cells[0].get_text(separator=\" \", strip=True)\n",
    "                    values = cells[1].find_all(\"span\", class_=\"value_StyledValue__KCmTz\")\n",
    "                    value_texts = [v.get_text(\" \", strip=True) for v in values]\n",
    "                    value = \", \".join(value_texts).replace(\"\\xa0\", \" \")\n",
    "                    data.get(name).update({table_name + \"  \" + key : value})\n",
    "        driver.delete_all_cookies()\n",
    "    except:\n",
    "        # if something goes wrong / or the site couldn't be loadet out of some reason\n",
    "        # not the nicest way, but the last run didn't print any problems anyway\n",
    "        print(f\"There was a Problem with: {i} {url}\")\n",
    "        pass\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of the dict\n",
    "print(list(data.keys())[0])\n",
    "data[list(data.keys())[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the dict to DataFrame and put the name in a new line\n",
    "df = pd.DataFrame.from_dict(data, orient='index')\n",
    "df = df.reset_index().rename(columns={\"index\": \"product_name\"})\n",
    "print(df.shape)\n",
    "# save as csv\n",
    "df.to_csv(\"robot_vacuums.csv\", index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".CIP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
